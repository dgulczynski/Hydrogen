% Opcje klasy 'iithesis' opisane sa w komentarzach w pliku klasy. Za ich pomoca
% ustawia sie przede wszystkim jezyk i rodzaj (lic/inz/mgr) pracy, oraz czy na
% drugiej stronie pracy ma byc skladany wzor oswiadczenia o autorskim wykonaniu.
\documentclass[declaration,shortabstract]{iithesis}

\usepackage[utf8]{inputenc}

%%%%% DANE DO STRONY TYTUŁOWEJ
% Niezaleznie od jezyka pracy wybranego w opcjach klasy, tytul i streszczenie
% pracy nalezy podac zarowno w jezyku polskim, jak i angielskim.
% Pamietaj o madrym (zgodnym z logicznym rozbiorem zdania oraz estetyka) recznym
% zlamaniu wierszy w temacie pracy, zwlaszcza tego w jezyku pracy. Uzyj do tego
% polecenia \fmlinebreak.
\polishtitle    {Inferencja typów dla języka programowania z efektami algebraicznymi}
\englishtitle   {
    Implementation of type inference \fmlinebreak
    for a programming language \fmlinebreak
    with algebraic effects
}


\englishabstract{
Algebraic effects and handlers are a novel and powerful programming construct that
mediates between manageable side effects and meaningful type safety.

Based on the recent works in this field, we proposed a calculus and a type-and-effect system
that enabled us to implement ML-style polymorphism for decidable type inference.
The inference algorithm we present has proven to be a non-trivial
refinement of a classical, syntax-oriented algorithm $\mathbb{W}$,
due to the introduced subtyping relation, additional calculus constructs and extended typing rules.
Theoretical description of our algorithm is accompanied by practical implementation.
}
\polishabstract {
Handlery i efektami algebraicznymi to nowoczesna i skuteczna konstrukcja programistyczna,
pośrednicząca pomiędzy wykorzystaniem efektów ubocznych a bezpiecznym systemem typów.

Na podstawie ostatnich prac w tej dziedzinie, zaproponowaliśmy rachunek oraz system typów i efektów, który umożliwił nam wdrożenie polimorfizmu w stylu ML dla rozstrzygalnej inferencji typów. Przedstawiony przez nas algorytm inferencji jest nietrywialnym rozszerzeniem klasycznego algorytmu syntaktycznej inferencji typów  $\mathbb{W}$, ze względu na wprowadzoną relację podtypowania, dodatkowe konstrukcje języka i rozszerzone reguły typowania. Teoretycznemu opisowi naszego algorytmu towarzyszy praktyczna implementacja.
}

% w pracach wielu autorow nazwiska mozna oddzielic poleceniem \and
\author         {Dominik Gulczyński}
% w przypadku kilku promotorow, lub koniecznosci podania ich afiliacji, linie
% w ponizszym poleceniu mozna zlamac poleceniem \fmlinebreak
\advisor        {dr Filip Sieczkowski}
%\date          {}                     % Data zlozenia pracy
% Dane do oswiadczenia o autorskim wykonaniu
\transcriptnum {299391}                     % Numer indeksu
\advisorgen    {Sieczkowskiego} % Nazwisko promotora w dopelniaczu
%%%%%

%%%%% WLASNE DODATKOWE PAKIETY
%
%\usepackage{graphicx,listings,amsmath,amssymb,amsthm,amsfonts,tikz}
\usepackage{lipsum}
\usepackage{amsmath,amssymb,amsthm,mathpartir,mathtools,listings, proof, semantic, wasysym}
%
%%%%% WŁASNE DEFINICJE I POLECENIA
%
\theoremstyle{definition} \newtheorem{definition}{Definition}[section]
%\theoremstyle{remark} \newtheorem{remark}[definition]{Observation}
%\theoremstyle{plain} \newtheorem{theorem}[definition]{Theorem}
%\theoremstyle{plain} \newtheorem{lemma}[definition]{Lemma}
%\renewcommand \qedsymbol {\ensuremath{\square}}
%
\newcommand{\types}[4][\Gamma;\Theta]{\ensuremath{{{#1} \vdash {#2} \: : {#3}/{#4}}}}
\newcommand{\gens}[6][\Gamma;\Theta]{\ensuremath{{{#1} \vdash_\textbf{Gen} {#2} \: : {#3}/{#4}} \rightsquigarrow {#5};{#6}}}
\newcommand{\arrow}[3]{{#1}\rightarrow_{#2}{#3}}
\newcommand{\lam}[1][x]{\ensuremath{\lambda #1.\:}}
\newcommand{\fun}[1][f,\:x]{\ensuremath{\textbf{fun } #1.\:}}
\newcommand{\optypes}[5][\Theta]{\ensuremath{{#1}\vdash{#2}_{#3} : {#4} \Rightarrow {#5}}}
\newcommand{\ops}[2]{\optypes{op}{a}{#1}{#2}}
\newcommand{\fresh}[1]{\ensuremath{\operatorname{fresh}({#1})}}
\lstset{
 language=caml,
 columns=[c]fixed,
 basicstyle=\small\ttfamily,
 keywordstyle=\bfseries,
 upquote=true,
 commentstyle=,
 breaklines=true,
 showstringspaces=false,
 stringstyle=\color{blue},
 literate={'"'}{\textquotesingle "\textquotesingle}3,
%   basicstyle=\ttfamily,
  mathescape
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
%%%%% POCZĄTEK ZASADNICZEGO TEKSTU PRACY %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}

Programming languages that enjoy \textit{static type safety} require that
each variable and expression is given a \textit{type}.
Assigning values to variables of different types usually results in a compile error.

Type inference method was designed to aid the process of \textit{type-checking}
and to ease the lives of programmers. It is a process of \textit{deducing}
the type of an expression, by the analysis of its structure and the environment
in which we carry out the inference process. 
Many programming languages use it to free programmers from the burden of writing
explicit type annotations, which makes code easier to read and modify.
For example, in programming language C$\sharp$ programmer may
use the \texttt{var} keyword to omit the type in variable declaration.

A \textit{complete} type inference system can deduce a type for every
expression without the need for any type annotations.
Most popular of such type systems, called \textit{Hindley-Milner}, is based on the
works of J. Roger Hindley\cite{hindley},
Robin Milner, who presented \textit{the} type inference algorithm $\mathbb{W}$\cite{milner},
and Luis Damas\cite{principal}.
Their work is a corner stone of type theory for programming languages
and the basis for type systems and type inference present in most of the statically typed
functional programming languages.

Type systems like System-F or Hindley-Milner's, based on the \textit{lambda calculus}
are great mathematical models of describing pure computations and types of pure terms concisely.
Yet, a lot of real-life programs heavily use side effects,
be it writing to memory, performing I/O actions, or mutating state, and those systems cannot express them.
There are different ways of describing side effects in both scientific literature
and real world implementations of programming languages.
Some languages do not restrict side effects at all, like OCaml,
while others have their unique ways of expressing side effects,
like monadic actions in Haskell.
Finally there are so called type-and-effect systems.
One of the earliest works by Talpin and Jouvelot\cite{talpin}
in this research area describe studying effects as an additional way of reasoning about programs:
while the type of an expression tells us about the \textit{results} of computation,
the effects describe \textit{how} it computes.


The particular system and calculus (which will from now on be referred to as \textit{the original calculus}) presented by Biernacki et al\cite{binders-labels}
uses \textit{algebraic effects} to describe programming with computational effects.
An effect is defined by a set of available operations (defined by its \textit{signature})
and a handler that provides its implementation.
Operators are meaningless on their own, acting much like ordinary functions
calling the handler's body, while the handler itself defines the way operation calls execute.

One could think of effects as a generalization of exceptions:
calling an operator corresponds to throwing an exception,
and surround an expression with a handler, corresponds to \textit{try \{...\} catch \{...\}}
construct known in some form in many mainstream languages.
But, in most of those languages exceptions are not \textit{resumable},
meaning that once we \textit{leave} the inner expression of a handler,
by calling one of its operators, we cannot continue the computation of
handled expression.
In \textit{the original calculus}, the programmer is free
to use the \textit{continuation} however they like when programming handlers.

We transcribe \textit{the original calculus} to the world of ML programming
by presenting modified type-and-effect system and corresponding calculus,
together with type inference algorithm for it.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Calculus}
The calculus we present and analyze in this work
is a subset of the one described by Biernacki et al\cite{binders-labels},
 adjusted to match the style of \textit{ML-the-calculus} and \textit{ML-the-type-system}\cite{emlti}.
It is an extension of standard \textit{call-by-value lambda calculus with let} by effect handlers and operators. 
%%%% CALCULUS' SYNTAX %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{align*} 
x , \dots & & \text{(term variables)}
\\ 
\alpha , \dots& & \text{(quantified variables)}
\\ 
t , \dots & & \text{(type variables)}
\\ 
\epsilon , \dots & & \text{(effect variables)}
\\ 
a , \dots & & \text{(instance variables)}
\\ 
\tau \Coloneqq &\
\alpha \mid t \mid \textbf{Unit} \mid \textbf{Int} \mid \arrow{\tau}{\varepsilon}{\tau}
\mid \forall (a : \sigma) .\: \tau
 & \text{(types)}
\\
\pi \Coloneqq &\ \forall \alpha.\:\pi \mid \tau &\text{(type schemes)}
\\ 
\varepsilon \Coloneqq &\
\alpha \mid \epsilon \mid a \mid \varepsilon \cdot \varepsilon 
 & \text{(effects)}
\\ 
\sigma \Coloneqq &\ \textbf{Error} \mid \textbf{State } \tau& \text{(signatures)}
\\
e \Coloneqq &\ x \mid () \mid n \mid \lam e \mid \fun e \mid e \: e \mid \textbf{let } x = e \: \textbf{ in } \: e & \text{(terms)}
\\
 & \;\:\ \mid \lam[(a : \sigma)] e \mid e \: a \mid op_a \: e \mid \textbf{handle}_a \: e\: \{h; \textbf{return } x. e\}
\\
op \Coloneqq &\ \textbf{Raise} \mid \textbf{Get} \mid \textbf{Put} & \text{(operators)} 
\\ 
h \Coloneqq &\ [(\textbf{Raise}, x, k. e)] \mid [(\textbf{Get}, x, k. e);\ (\textbf{Put}, x, k. e)]& \text{(handlers)}
\end{align*}
Terms (or expressions) are given by:
\begin{itemize}
    \item variables, bound by abstractions, let-expressions, handlers, or environment $\Gamma$,
    \item constants: $()$ and $n \in \mathbb{C}$,
    \item abstractions: anonymous functions $\lam e$ with argument $x$ and body $e$ and recursive functions denoted $\fun e$,
    \item instance abstraction: $\lam[(a:\sigma)]e$, that lets programmers write code
    unspecific to certain effect \textit{instance}, but capable of working with any
    instance of specified signature,
    \item applications: $e_1\:e_2$ and $e\:a$, for applying arguments to respective abstractions, 
    \item let-construct: $\textbf{let } x = e_1 \textbf{ in } e_2$, which first evaluates body of $e_1$,
    and bounds its value to variable $x$ in expression $e_2$,
    \item operation calls: $op_a\:e$ calling the $op$ operator handler of instance $a$ with value
    of $e$,
    \item handlers: $\textbf{handle}_a \: e\: \{h;\textbf{return } x. e_r\}$ of instance $a$, 
    which provides meaning to operators: calling $op_a\:e_x$, executes body $e_{op}$ of
    a construct $(op, x, k.\:e_{op})$ defined in handler $h$.
\end{itemize}

Instead of allowing effects of arbitrary signatures, we limited them to instances
of either $\textbf{Error}$ or  $\textbf{State }\tau$.
Arbitrary signatures could be dealt with in similar fashion as \textit{ADTs}
(algebraic data types) and are \textit{orthogonal} to type inference.

For the \textit{complete type inference} we need to limit the system to \textit{prenex}
polymorphism and thus we differentiate between \textit{monomorphic} types and \textit{polymorphic}
type schemes (which are types proceeded by $\forall$-quantifier followed by $\alpha$ variables).
Accordingly we have dropped the type-lambdas ($\Lambda$) from calculus in favor of the
polymorphic let-construct.

Notice how each expression, type and effect of our calculus can be expressed in terms of \textit{original calculus}. The semantics of our calculus strictly follow the rules defined in the original paper \textit{Binders by Day, Labels by Night} by Biernacki et al\cite{binders-labels}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Type system}
We have tweaked the type-and-effect system of \textit{the original calculus}
to match the restrictions required by the classical methods of type inference.
Thus, we introduce \textit{unification variables} for both types and effects
(written $t$ and $\epsilon$, respectively) to denote ``yet to be determined'' types and effects.
During the type inference we will \textit{generate} these unification variables,
and deduce rigid types for them, as described in \hyperlink{chapter.4}{Chapter 4}.

Judgement $\types[\Gamma, \Theta]{e}{\tau}{\varepsilon}$ states that in environments $\Gamma$ (assigning types to variables),
and $\Theta$ (assigning signatures to instances), term $e$ \textit{inhabits} syntactical type and effect $\tau / \varepsilon$ (which means that computing $e$ would yield a value of type $\tau$ and possibly cause effect $\varepsilon$).

%%%% SYNTACTIC TYPE JUDGEMENT %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
$$
\inference{}{\types{()}{\textbf{Unit}}{\iota}}
\quad
\inference{}{\types{n}{\textbf{Int}}{\iota}}{}
$$
\setlength{\jot}{10pt}
There are two \textit{base types} \textbf{Unit} and \textbf{Int} for constants.

$$
\inference{(x : \pi) \in \Gamma & \pi[\vec{\tau_\alpha} / \vec{\alpha}] = \tau}{\types{x}{\tau}{\iota}}
$$
Judgement for variables follows the usual let-polymorphism typing, 
where variables bound by let clauses are generalized and need to be instantiated.
Variables do not cause effects as only the value is assigned to them,
while the effects caused by their computation (if any occur)
are bound to the term which introduced that variable.

$$
\inference{
    \types[\Gamma,(x : \tau_1); \Theta]{e}{\tau_2}{\varepsilon} 
}{
    \types{\lam e}{\arrow{\tau_1}{\varepsilon}{\tau_2}}{\iota}
}
\quad
\inference{
    \types[\Gamma, (f : \arrow{\tau_1}{\varepsilon}{t_2}),(x : \tau_1); \Theta]{e}{\tau_2}{\varepsilon} 
}{
    \types{\fun e}{\arrow{\tau_1}{\varepsilon}{\tau_2}}{\iota}
}
$$
The \textit{type constructor} $\rightarrow$ is used to type abstractions,
where type $\arrow{\tau_1}{\varepsilon}{\tau_2}$ is given to functions
that when applied with input of type $\tau_1$, computing the function's body $e$ produces some output of type $\tau_2$, possible causing effect $\varepsilon$

$$
\inference{
    \types{e_1}{\arrow{\tau_2}{\varepsilon}{\tau}}{\varepsilon_1} &
    \types{e_2}{\tau_2}{\varepsilon_2} &
}{
    \types{e_1\:e_2}{\tau}{\varepsilon_1 \cdot \varepsilon \cdot \varepsilon_2}
}
$$
Accordingly, the effect of application combines effects of: computing the left-hand side term,
effect that ``hangs'' on it's arrow type, and the effect of computing the right-hand side term.

\begin{gather*}
\begin{gathered}
\inference{
    \types{e_1}{\tau_1}{\iota} &
    \operatorname{gen}(\Gamma, \tau_1) = \pi &
    \types[\Gamma,(x:\pi);\Theta]{e_2}{\tau}{\varepsilon}
}{
    \types{\textbf{let } x = e_1 \textbf{ in } e_2}{\tau}{\varepsilon}
}
\\
\inference{
    \types{e_1}{\tau_1}{\varepsilon_1} & 
    \varepsilon_1 \neq \iota & 
    \types[\Gamma,(x:\tau_1);\Theta]{e_2}{\tau}{\varepsilon}
}{
    \types{\textbf{let } x = e_1 \textbf{ in } e_2}{\tau}{\varepsilon}
}
\end{gathered}
\end{gather*}
As usually in let-polymorphism schemes, we \textit{generalize} the type derived for $e_1$ before
we add it to the environment in which we derive type for $e_2$.
This is achieved by the function
$\operatorname{gen}(\Gamma, \tau) = \forall \vec\alpha.\:\tau \text{ where }
\vec\alpha = \operatorname{freevars}(\tau) \setminus \operatorname{freevars}(\Gamma)$.
We restrict generalization to only $pure$ terms,
i.e. such that their computation would cause no effects.

$$
\inference{
    \types[\Gamma; \Theta, (a : \sigma)]{e}{\tau}{\iota}
}{
    \types{\lam[(a : \sigma)]. e}{\forall (a : \sigma) . \tau}{\iota}
}
\quad
\inference{
    \types{e}{\forall (a : \sigma) . \tau}{\iota} & (b:\sigma) \in \Theta
}{
    \types{e\:b}{\tau[b/a]}{\iota}
}
$$
For handling operators of distinct occurrences of the same signature,
i.e many cells of memory via $\textbf{State }\tau$,
we differentiate them by assigning them different instances.
\textit{Instance abstractions} allow us to write code that works not only with one instance of particular signature,
but with any that is supplied using \textit{instance application} of instances bound by handlers or other instance lambdas.

$$
\inference{
    \ops{\tau_e}{\tau} &
    \types{e}{\tau_e}{\varepsilon} 
}{
    \types{op_a \: e}{\tau}{a \cdot \varepsilon}
}
$$
Operators of type $\tau_1 \Rightarrow \tau_2$ expect arguments of type $\tau_1$
and return value of type $\tau_2$. Type of $e$ must match the operator type.
Effect of computing $e$ is extended by $\{a\}$.

$$
 \inference{
    \types{h}{\sigma \triangleright \tau}{\varepsilon} &
    \types[\Gamma;\Theta,(a:\sigma)]{e}{\tau'}{a \cdot \varepsilon} &
    \types[\Gamma,(x:\tau');\Theta]{e_r}{\tau}{\varepsilon}
}{
    \types{\textbf{handle}_a \: e\: \{h;\textbf{return } x. e_r\}}{\tau}{\varepsilon}
}
$$
For type-checking handler body, there's a separate relation $\vdash \ : \ \triangleright$, explained below.
Types of all \textit{execution paths} of the handler must match,
whether the value returns from within handler body
or via return expression $e_r$ (where value of evaluating $e$ gets bound to $x$).

Finally, we allow every type and effect to \textit{grow} as needed:
$$
\inference{
    \types{e}{\tau'}{\varepsilon'} & \tau' <: \tau & \varepsilon' <: \varepsilon
}{
    \types{e}{\tau}{\varepsilon}
}
$$

\begin{gather*}
\begin{gathered}
\inference{
    \types[\Gamma,(x:\textbf{Unit}), (k:\arrow{\tau'}{\varepsilon}{\tau});\Theta]{e'}{\tau}{\varepsilon}
}{ 
    \types{ [(\textbf{Raise}, x, k. e')]}{\textbf{Error}\triangleright \tau}{\varepsilon}
}
\\
\inference{
\types[\Gamma,(x:\textbf{Unit}), (k:\arrow{\tau'}{\varepsilon}{\tau});\Theta]{e'}{\tau}{\varepsilon} \\
\types[\Gamma,(x:\tau'),(k:\arrow{\textbf{Unit}}{\varepsilon}{\tau'});\Theta]{e''}{\tau}{\varepsilon} 
}{
\types{ [(\textbf{Get}, x, k. e');(\textbf{Put}, x, k. e'')]}{\textbf{State }\tau'\triangleright \tau}{\varepsilon} 
}
\end{gathered}
\end{gather*}
Supplying the \textit{continuation} $k$ with some value $v$ continues evaluation of
the expression surrounded by the handler,
with $v$ substituted in place of operation call,
thus in the body of operator handler of type $\tau_1\Rightarrow\tau_2$, $k$ is given type
$\arrow{\tau_2}{\varepsilon}{\tau}$ and $x$ is given $\tau_2$.
In a sense $k$ acts just like an ordinary function, and programmer may use it
in many different ways or not even use it all, returning a value straight from the handler code.
However they choose to do so, the type-and-effect of handlers expression must match the type
of the whole expression in which the handler was used.

\begin{gather*}
\begin{gathered}
\inference{
    (a : \textbf{Error}) \in \Theta
}{
    \optypes{\textbf{Raise}}{a}{\textbf{Unit}}{\tau}
}
\quad 
\inference{
    (a : \textbf{State }\tau) \in \Theta
}{
    \optypes{\textbf{Put}}{a}{\textbf{Unit}}{\tau}
}
\quad 
\inference{
    (a : \textbf{State }\tau) \in \Theta
}{
    \optypes{\textbf{Get}}{a}{\tau}{\textbf{Unit}}
}
\end{gathered}
\end{gather*}
Relation $\optypes{op}{a}{\tau_1}{\tau_2}$ simply finds the signature
$\sigma$ bound to instance $a$ in environment $\Theta$ and returns the operator's type.    

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Subtyping}
%%%% SUBTYPING RULES %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{gather*}
\begin{gathered}
\inference{}{\tau <: \tau}
\quad
\inference{
    \tau_1' <: \tau_1 & \varepsilon <: \varepsilon' & \tau_2 <: \tau_2'
}{
    \arrow{\tau_1}{\varepsilon}{\tau_2} <: \arrow{\tau_1'}{\varepsilon'}{\tau_2'}
} 
\quad
\inference{}{\varepsilon <: \varepsilon}
\quad
\inference{
    \varepsilon <: \varepsilon'
}{
    \varepsilon <: \varepsilon' \cdot \varepsilon''
}
\end{gathered}
\end{gather*}
\setlength{\jot}{3pt}
The subtyping rule we propose is \textit{structural}, meaning that only types of
\textit{matching shape} are related.
However, while leaves containing types must be equal, we allow effects to differ as long as they are related.
Notice that the $\rightarrow$ is contravariant to subtyping relation
(in the premise of rule for $\rightarrow$, the order of relation on $\tau_1$ and $\tau_1'$ is reversed).

Although the effect constructor $\cdot$ we used to define effects in 
\hyperlink{chapter.1}{Chapter 1} forms binary trees,
we implicitly use an equivalence relation on effects that interprets
these trees as finite sets.
To be precise, we treat effects $\varepsilon_1$ and $\varepsilon_2$ as equivalent
as long as each leaf in the tree of $\varepsilon_1$ 
also appears somewhere in the tree of $\varepsilon_2$.
For example, we have $a \equiv a\cdot a$ and $b \cdot a \cdot b \equiv a \cdot b$.

The subtyping plays a vital role in usability of the calculus.
Consider a term $f$, some higher order function that does some calculation
when applied with some arithmetical function.

Consider a term $f$, that takes as an arithmetical function as argument,
which is allowed to cause no effects other than failing to compute via effect $e$.
$$
\types[\emptyset, (e: Error)]
    {f}
    {\arrow{(\arrow{\textbf{Int}}{e}{\textbf{Int}})}{e}{\textbf{Int}}}
    {\iota}
$$
Thanks to the subtyping relation, we could apply some pure function to $f$
and the application would still type-check.
On the other hand, it would be undesirable if we could supply a term that expects
a pure function with an effectful one.
Clearly this property gives us flexibility, while keeping effects under control.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Principal type}
In ML type system, the \textit{principal type} property states that there exists
a \textit{most general} type for any correct program\cite{principal}. 
A type scheme $\pi$ is called \textit{principal} if any other type that
could be given to $e$ is an \textit{instantiaton} of it.

For example, in ML calculus, term $e = \lam x$ could be given type
$\arrow{\textbf{Unit}}{}{\textbf{Unit}}$ or $\arrow{(\arrow{\textbf{Int}}{}{\textbf{Int}})}{}{\arrow{\textbf{Int}}{}{\textbf{Int}}}$,
but clearly any correct type we could think of would not be more general than $\forall \alpha . \arrow{\alpha}{}{\alpha}$, which is in fact, the principal type of $e.$

\theoremstyle{definition}
\begin{definition}{Principal Type}
\\
In our type system, $\pi$ is a principal type of $e$ in environments $\Gamma$, $\Theta$ iff
$$ \forall \vec{\tau}. \: \types{e}{\pi[\vec{\tau} / \vec{\alpha}]}{\iota} \;\wedge\;
    \forall \pi'.\:\exists \vec{\tau'}.\: \types{e}{\pi'[\vec{\tau'} / \vec{\alpha'}]}{\iota} \;\wedge\; 
    \pi[\vec{\tau} / \vec{\alpha}] <: \pi'[\vec{\tau'} / \vec{\alpha'}]
$$
\end{definition}

We talk about this property more in \hyperlink{chapter.6}{Chapter 6}.
For now we will give an example of how to approximate a principal type.
Consider the two type schemes that could be assigned to a term $\textit{compose} = \lam[f]\lam[g]\lam f (g\:x) $, representing function composition:
\begin{align*}
    \pi_1\Coloneqq  \forall\alpha,\beta. &\:
    \arrow{(\arrow{\tau_b}{\alpha}{\tau_c})}{}{
        \arrow{(\arrow{\tau_a}{\beta}{\tau_b})}{}{
                \arrow{\tau_a}{\alpha \cdot \beta}{\tau_c}}} 
    \\
    \pi_2\Coloneqq  \forall\gamma      . &\:
    \arrow{(\arrow{\tau_b}{\gamma}{\tau_c})}{}{
        \arrow{(\arrow{\tau_a}{\gamma}{\tau_b})}{}{
                \arrow{\tau_a}{\gamma}{\tau_c}}} 
\end{align*}

At first glance, it may look like $\pi_1$ is the ``correct'' type for \textit{compose}, as it seems more natural,
 i.e. given functions $f$ causing effect $\varepsilon_f$, and $g$ causing $\varepsilon_f$, $\textit{compose}\:f\:g$
is a function that would apply them both, so it clearly must be causing effect $\varepsilon_f \cdot \varepsilon_g$.

While this reasoning is sound, we are interested in deriving the most concise type possible. Let's see if there exists $\varepsilon_h$ such that $\pi_1$ instantiated with arbitrary effects $\varepsilon_f$, $\varepsilon_g$ subtypes $\pi_2$ instantiated with $\varepsilon_h$:
\begin{align*}
    \pi_1[\varepsilon_f, \varepsilon_g / \alpha, \beta] <: & \: \pi_2 [\varepsilon_h / \gamma]
    \\ \iff \\ 
    \arrow{(\arrow{\tau_b}{\varepsilon_f}{\tau_c})}{}{
        \arrow{(\arrow{\tau_a}{\varepsilon_g}{\tau_b})}{}{
                \arrow{\tau_a}{\varepsilon_f \cdot \varepsilon_g}{\tau_c}}} <: & \:
    \arrow{(\arrow{\tau_b}{\varepsilon_h}{\tau_c})}{}{
        \arrow{(\arrow{\tau_a}{\varepsilon_h}{\tau_b})}{}{
                \arrow{\tau_a}{\varepsilon_h}{\tau_c}}}
    \\ \iff \\ 
    \varepsilon_h <: \varepsilon_f          \;\wedge\; 
    \varepsilon_h <: &  \: \varepsilon_g    \;\wedge\; 
    \varepsilon_f \cdot \varepsilon_g <: \varepsilon_h
\end{align*}
Clearly, $\pi_1[\varepsilon_f, \varepsilon_g / \alpha, \beta] <: \pi_2 [\varepsilon_h / \gamma]$
does not hold for $\varepsilon_f$ and $\varepsilon_g$ other than $\iota$, thus $\pi_1$ cannot be a principal type of $e$.
On the other hand, if we were to check if for arbitrary $\varepsilon_h$ there exist $\varepsilon_f$ and $\varepsilon_g$ such that
$\pi_2 [\varepsilon_h / \gamma] <: \pi_1[\varepsilon_f, \varepsilon_g / \alpha, \beta]$, we would need to find witnesses for such formula:
$$ \varepsilon_f <: \varepsilon_h          \;\wedge\; 
    \varepsilon_g <: \varepsilon_h    \;\wedge\; 
     \varepsilon_h <: \varepsilon_f \cdot \varepsilon_g $$
Clearly if we choose $ \varepsilon_f = \varepsilon_g = \varepsilon_h$, it is satisfied, which means that $\pi_2$ is indeed more general $\pi_1$.
We designed our inference algorithm with this intuition in mind and $\pi_2$ is the result that our implementation actually infers, as we show in \hyperlink{section.4.4}{Chapter 4}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Inference algorithm}
The algorithm we present loosely follows original algorithm $\mathbb{W}$ and executes in two distinct phases:
\begin{enumerate}
    \item Constraint gathering: the algorithm traverses the structure of given expression,
    generating sub-expressions' types, effects and constraints,
    \item Constraint solving: the algorithm builds a substitution that resolves gathered constraints,
    in respect to the laws of type checking.
\end{enumerate}
In practice, these phases are often interleaved, as even in pure \textit{let-polymorphism} type interference when handling expression $\textbf{let } x = e_1 \textbf{ in } e_2$  we need to solve constraints regarding the inferred type of $e_1$, so we can \textit{generalize} it before adding it to the environment for inferring $e_2$.

\begin{align*}
 C & \Coloneqq  
    \emptyset  \mid  \{\tau <: \tau\} \mid \{\varepsilon <: \varepsilon\} \mid C \cup C 
    & \text{(constraints)}
\\https://www.overleaf.com/project/5f3c5361269a720001d5cf2d
 S & \Coloneqq 
    {id}  \mid  [t \mapsto  \tau] \mid [\epsilon \mapsto  \varepsilon] \mid S \circ S 
    &  \text{(substitutions)}
\end{align*}
Finally, the key difference in our algorithm arises.
In the standard HM algorithm and its derivatives that do not support subtyping, 
the constraints are rather of form $\{x = y\}$.
Such constraints are \textit{simple}, as they undoubtedly require $x$ and $y$
to be unified. Instead, we have constraints of type $\{x <: y\}$.
Presence of subtyping relation in type inference process proves to be
problematic, as described by francois in his work.
well well well

For base types, it means the same, but for effects is hella more complicated and ya know.

are a way of describing necesarry conditions that need to be satisfied
for the expresion and inferred type to type-check.


A substitution is a mapping from type and effect \textit{unification variables} to
inferred types or effects, respectively.
We will write $S[t \mapsto \tau]$ for $[t\mapsto \tau] \circ S$.
Substitution $id$ is simply an identity function.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Remarks on effect unification}
With combining effect \textit{unification variables} and constraint solving, a problem arises.
Consider constraint $a <: \epsilon_1 \cdot \epsilon_2$,
for some instance variable $a$ and some effect unification variables $\epsilon_1$, $\epsilon_2$.
To resolve such constraint we have a few viable options:
\begin{enumerate}
    \item Expand $\epsilon_1$ with $a$.
    \item Expand $\epsilon_2$ with $a$.
    \item Expand both $\epsilon_1$ and $\epsilon_2$ with $a$.
\end{enumerate}
But how would we choose one over the other? Maybe one of those makes the program ill-typed, while the others do not? What if there's more than two unification variables? Clearly such constraints are undesirable.

To tackle this problem, in our algorithm we permit no more than one effect
\textit{unification variable} or \textit{quantified variable} in effects.
Thus the effects are defined differently than in Chapter 1:
\begin{align*} 
    \varepsilon & \Coloneqq
    I \mid 
    I * \alpha \mid 
    I * \epsilon  &  \text{(effects)}
\\
    I & \Coloneqq \iota \mid \{a\} \mid I \cup I \mid I \setminus I
    \quad & \text{(sets of instances)}
\end{align*}
So an effect is either just a finite set of instances, or a union of one with either effect \textit{unification variable} or \textit{generalized} effect variable.
If we think about effects as sets,
then the subtyping relation of effects simply boils down to set inclusion.
Notice how every effect defined by the new grammar is expressible in the previous one as well.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Generating constraints}
As in the \textit{syntax-directed} algorithm $\mathbb{W}$, to infer the type for a given term,
we build it by working \textit{bottom-up} from the leaves through the whole expression tree.
To this end, we have defined judgement $\gens{e}{\tau}{\varepsilon}{C}{S}$ that
states in the the premise what conditions need to be satisfied for deducing type, 
and under what constraints $C$ and substitution $S$ shall we interpret it.

We chose to present  $\vdash_\textbf{Gen}$ typing rules as close as possible
to the practical inference algorithm.
Hence, we heavily use \textit{unification variables}
(denoted $t$ for types and $\epsilon$ for effects),
and the judgement should be thought of as a semi-formal description of
type and constraints generating process (working bottom-up) rather than
a type checker ($\vdash$ working top-down).

It is important that we ``return'' not only constraints, but also a substitution, 
because some constraints may have been already resolved in
one sub-tree of the term, effectively changing the environment,
and we need to take it into account while inferring the other sub-trees.
We abstract solving constraints $C$ in environment $\Gamma$ (under substitution $S$) to
a high-level functions \textit{solve} and \textit{solve'} which return a reduced set of constraints $C'$ and
a new substitution $S'$. 
\theoremstyle{definition} 
\begin{definition}{Syntactic type soundness}
   $$
   \begin{aligned}
    \gens{e}{\tau}{\varepsilon}{C}{S} \implies& &\\
    \operatorname{solve}(\Gamma,\tau/\varepsilon, C, S) = \emptyset; S' & \implies & \\
      &\types[S'\Gamma;S'\Theta]{e}{S'\tau}{S'\varepsilon} &
    \end{aligned}
    $$
\end{definition}
We would say that an inference algorithm enjoys \textit{syntactic type soundness}
if the type generated by it (after we have solved all the constraints and built the substitution)
for the given term checks by the syntactic rules we have defined in \hyperlink{chapter.3}{chapter 3}.
Proof of \textit{soundness} our algorithm is left for \hyperlink{chapter.6}{future work}.

\setlength{\jot}{12pt}
%%%% GENERATIVE JUDGEMENT %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{gather*}
    \inference {}{\gens{()}{\textbf{Unit}}{\iota}{\emptyset}{id}}
    \quad
    \inference {}{\gens{n}{\textbf{Int}}{\iota}{\emptyset}{id}} {}
\\
    \inference {
        (x : \pi) \in \Gamma & \operatorname{inst}(\pi) = \tau
    }{
        \gens{x}{\tau}{\iota}{\emptyset}{id}
    }
\end{gather*}
As environment $\Gamma$ contains type schemes rather than types, before we return the type of $x$,
we switch out every variable quantified in $\pi$ to a fresh unification variable
via function \textit{inst}.
\begin{gather*}
     \inference{
        \gens[\Gamma, (x : t); \Theta]{e}{\tau}{\varepsilon}{C}{S} &
        \fresh{t}
    }{
        \gens{\lam e}{\arrow{t}{\varepsilon}{\tau}}{\iota}{C}{S}
    }
\\
    \inference{
        \fresh{t_1} &
        \fresh{\epsilon} &
        \fresh{t_2} \\
        \gens[\Gamma, (f : \arrow{t_1}{\epsilon}{t_2}),(x : t_1); \Theta]{e}{\tau}{\varepsilon}{C}{S}
    }{
        \gens{\fun e}{\arrow{t_1}{\epsilon}{t_2}}{\iota}
        {C \cup \{\varepsilon <: \epsilon, \tau <: t_2 \}}{S}
    }
\end{gather*}
For recursive functions, we must ensure that the actual type-and-effect of function's body $e$
subtypes one we have generated for $f$.

\begin{gather*}
\inference{
    \gens{e_1}{\tau_1}{\varepsilon_1}{C_1}{S_1} &
    \gens[S_1\Gamma;S_1\Theta]{e_2}{\tau_2}{\varepsilon_2}{C_2}{S_2} \\
    \fresh{t} &
    \fresh{\epsilon} &
    C = C_1 \cup C_2 \cup \{\varepsilon_1 <: \epsilon ,
    \varepsilon_2 <: \epsilon, \tau_1 <: \arrow{\tau_2}{\epsilon}{t} \}
}{
    \gens{e_1\:e_2}{t}{\epsilon}{C
    }{S_2 \circ S_1}
}
\end{gather*}
As mentioned earlier, we use the generated substitution $S_1$ to update the
environment for handling $e_2$.
Additional constraints ensure that the fresh effect variable we return is sybtyped by 
the effects of computing $e_1$, $e_2$, and the effect hung on the $\rightarrow$ type of $e_1$.

\begin{gather*}
\inference{
    \gens{e_1}{\tau_1}{\varepsilon_1}{C_1}{S_1} &
    \operatorname{solve}(\Gamma,\tau_1/\varepsilon_1, C_1, S_1) =  C; S \\
    S \varepsilon_1 = \iota &
    \operatorname{gen}(S\Gamma, S\tau_1) = \pi &
    \gens[S\Gamma,(x:\pi);S\Theta]{e_2}{\tau_2}{\varepsilon_2}{C_2}{S_2}
}{
    \gens{\textbf{let } x = e_1 \textbf{ in } e_2}{\tau_2}{\varepsilon_2}{C\cup C_2}{S_2 \circ S}
}
\\
\inference{
    \gens{e_1}{\tau_1}{\varepsilon_1}{C_1}{S_1} &
    \operatorname{solve}(\Gamma,\tau_1/\varepsilon_1, C_1, S_1) =  C; S \\
    S \varepsilon_1 \neq \iota &
    S \tau_1 = \tau &
    \gens[S\Gamma,(x:\tau);S\Theta]{e_2}{\tau_2}{\varepsilon_2}{C_2}{S_2} &
    \fresh{\epsilon}
}{
    \gens{\textbf{let } x = e_1 \textbf{ in } e_2}{\tau_2}{\epsilon}{C\cup C_2
    \cup \{ \varepsilon_1 <: \epsilon, \varepsilon_2 <: \epsilon\}}{S_2 \circ S}
}
\end{gather*}
Before we can handle $e_2$, we need to check if type generated for $e_1$ can be generalized.
To this end, we deploy the \textit{solve} function, and if the substitution returned
ensured us that computing of $e_1$ would cause no effects, we can safely generalize it via \textit{gen} function,
otherwise we take its effect into account, adding constraints regarding fresh variable $\epsilon$.
Again, $S$ is applied to environments for handling $e_2$.

\begin{gather*}
\inference{
    \gens[\Gamma; \Theta, (a : \sigma)]{e}{\tau}{\varepsilon}{C}{S} \\
}{
    \gens{\lam[(a : \sigma)] e}{\forall (a : \sigma) . \tau}{\iota}{
        C \cup \{ \varepsilon <: \iota \}   
    }{S}
}
\\
\inference{
    \gens{e}{\tau'}{\varepsilon}{C'}{S'} &
    \operatorname{solve'}(\Gamma, C', S') = C; S  \\
    S \tau' = \forall (a : \sigma) . \tau &
    S \varepsilon = \iota &
    (b:\sigma) \in S\Theta
}{
    \gens{e\:b}{\tau[b/a]}{\iota}{C}{S}
}
\end{gather*}
For instance abstraction, we have to make sure that the expression underneath it is pure.
With instance application, we need to \textit{know} that the type for $e$ is quantified by some instance $a$,
so we use the weaker \textit{solve'} function to find out its type, before substituting $a$ for $b$ in it.

\begin{gather*}
\inference{
    \Theta \vdash op_a : \tau_1 \Rightarrow \tau_2 &
    \gens{e}{\tau_e}{\varepsilon}{C}{S}
}{
    \gens{op_a \: e}{\tau_2}{a * \varepsilon}{C \cup \{\tau_e <: \tau_1 \}}{S}
}
\end{gather*}
Operators resemble functions, so we make sure that the type of $e$ subtypes the type for arguments of $op_a$.
Effect returned is the consequence of computing $e$ combined with $\{a\}$.

\begin{gather*}
\begin{gathered}
\inference{
\gens{h}{\sigma \triangleright t}{\epsilon}{C_h}{S_h} \\
\gens[S_h\Gamma; S_h\Theta, (a:S_h\sigma)]{e}{\tau}{\varepsilon}{C_e}{S_e} \\
\gens[S_e S_h \Gamma,(x:\tau); S_e S_h \Theta]{e_r}{\tau_r}{\varepsilon_r}{C_r}{S_r} \\
\fresh{t} & \fresh{\epsilon} &
C = C_h \cup C_e \cup C_r \cup\{\varepsilon <: a * \epsilon, \tau_r <: t, \varepsilon_r <: \epsilon \}
}{
\gens{\textbf{handle}_a \ e\ \{h;\textbf{return } x.\ e_r\}}{t}{\epsilon}{C}{S_r \circ S_e \circ S_h}
}
\end{gathered}
\end{gather*}
Unsurprisingly, rules for typing handlers looks rather complicated,
but they are nothing more than the rules from $\vdash$, extended with subtyping and propagation of constraints and substitution.
\begin{gather*}
\begin{gathered}
\inference{ 
\gens[\Gamma,(x:\textbf{Unit}), (k:\arrow{t}{\varepsilon}{\tau});\Theta]{e}{\tau'}{\varepsilon'}{C}{S} &
\fresh{t}
}{
\gens{ [(\textbf{Raise}, x, k.\ e')]}{\textbf{Error}\triangleright \tau}{\varepsilon}{C\cup\{\tau' <: \tau, \varepsilon' <: \varepsilon\}}{S}
}
\\
\inference{
\gens[\Gamma,(x:\textbf{Unit}), (k:\arrow{t}{\varepsilon}{\tau});\Theta] 
{e'}{\tau'}{\varepsilon'}{C'}{S'}
\\
\gens[S'\Gamma,(x:t), (k:\arrow{\textbf{Unit}}{\varepsilon}{\tau});S' \Theta]
{e''}{\tau''}{\varepsilon''}{C''}{S''} \\
C = C' \cup C'' \cup \{ \tau' <: \tau,  \varepsilon' <: \varepsilon, \tau'' <: \tau,  \varepsilon'' <: \varepsilon \} &
S = S'' \circ S' & \fresh{t}
}{
\gens{ [(\textbf{Get}, x, k. e');(\textbf{Put}, x, k. e'')]}{\textbf{State } t\triangleright \tau}{\varepsilon}{C}{S}
}
\end{gathered}
\end{gather*}
\setlength{\jot}{3pt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Solving constraints}
The constraint solving algorithm we present is divided in two sub-procedures:
\begin{enumerate}
    \item \texttt{solve\_simple\_contraints} $C$ $S$\\
    deals with both type- and effect-constraints behaving, much like the ordinary HM algorithm.
    As the rules for type subtyping are somewhat trivial, it can solve them efficiently and environment-agnostically. As we explain in the following subsection, there are some effect constraints that are \textit{non-trivial} and cannot be resolved so easily, so they are dealt with by the second procedure:
    
    \item \texttt{solve\_contraints\_within} $\Gamma$ $(\tau, \varepsilon)$ $C$ $S$ \\
    deals with effect-constraints regarding effect unification variables ($\epsilon$) occurring in $\Gamma$, $\tau$ and $\varepsilon$.
    The interesting constrains are of form $\{\epsilon_1 <: I * \epsilon_2\}$,
    as there are many substations that could satisfy one such constraint, but it is not obvious which one is \textit{best} regarding bigger picture.
    In following subsections we argue how our approach approximates the most general type.
    For formal methods, see \hyperlink{chapter.6}{future work}.
\end{enumerate}

\subsection{Simple constraints}
We call constraints solved by the first sub-procedure \textit{simple} as the substitution they induce is \textit{minimal},
meaning it is unambiguous that their premises \textit{must} hold for the whole to be correct. Constrains that are \textit{interesting} are \textit{irreducible} by \texttt{solve\_simple}.
Other constraints are either solved or reduced to the \textit{interesting} form.
Notice that this procedure resolves all constraints that regard types (which are trivial)
and we can use the substitution returned by it
to find out \textit{shapes} of types (and effects, to some extent) without \textit{worrying} whether we would break
some constraints by resolving them too quickly.

\begin{lstlisting}
let expand $\epsilon$ $I$ $S$= 
  if $I = \emptyset$ then $S$
  else $S[\epsilon \mapsto I * \epsilon']$ where $\fresh{\epsilon'}$
\end{lstlisting}
\begin{lstlisting}
let solve_simple $C$ $S$ = 
  match $C$ with
  | $\emptyset$        $\rightarrow$ $\emptyset; S$
  | $\{\tau_1 <: \tau_2\} \cup C$ $\rightarrow$
    match $S[\tau_1], S[\tau_2]$ with
    | $\tau_1'$, $\tau_2'$ when $\tau_1' = \tau_2'$ $\rightarrow$
      solve_simple $C$ $S$
    | $t$, $\tau$
    | $\tau$, $t$ $\rightarrow$
      solve_simple $C$ $S[t \mapsto \tau]$
    | $\arrow{\tau_1'}{\varepsilon_1}{\tau_1''}, \arrow{\tau_2'}{\varepsilon_2}{\tau_2''}$ $\rightarrow$
      solve_simple $\{\tau_1' <: \tau_2', \varepsilon_1 <: \varepsilon_2, \tau_1'' <: \tau_2''\}\cup C$ $S$
  | $\{\varepsilon_1 <: \varepsilon_2\} \cup C$ $\rightarrow$
    match $S[\varepsilon_1], S[\varepsilon_2]$ with
    | $I_1$, $I_2$ when $I_1 \subseteq I_2$ $\rightarrow$ solve_simple $C$ $S$
    | $\iota$, _ $\rightarrow$ solve_simple $C$ $S$
    | $I_1$, $I_2 * \epsilon$ $\rightarrow$
      solve_simple $C$ (expand $(I_1 \setminus I_2)$ $\epsilon$ $S$)
    | $I_1 * \epsilon_1$, $I_2 * \epsilon_2$ $\rightarrow$
      if $\epsilon_1 = \epsilon_2$ then solve_simple $C$ (expand $(I_1 \setminus I_2)$ $\epsilon_2$ $S$)
      else let $C';S'$ = solve_simple $C$ (expand $(I_1 \setminus I_2)$ $\epsilon_2$ $S$) 
            in $\{\epsilon_1 <: I_2 * \epsilon_2\}\cup C'; S'$
    | $I_1 * \epsilon <: I_2$ when $I_1 \subseteq I_2$$\rightarrow$
      let $C';S'$ = solve_simple $C$ $S$ 
        in $\{\epsilon <: I_2\}\cup C'; S'$
\end{lstlisting}

We can now define function $\operatorname{solve'}$ from $ \vdash_\textbf{Gen} $ relation:
$$ \operatorname{solve'}(C, S) = \texttt{solve\_simple}\:C\:S $$ 

\subsection{Interesting constraints}
What makes constraints like $(\epsilon_1 <: I_2 * \epsilon_2)$ interesting,
is that there are many plausible substitutions that satisfy it.
For every set of instances $I_1 \subseteq I_2$, substitutions $[\epsilon_1 \mapsto I_1]$ and
$[\epsilon_1 \mapsto I_1 * \epsilon_2]$ obviously resolve the constraint, but clearly some substitutions are better than others. 

As discussed in previous chapter, $\rightarrow$ type constructor is \textit{contravariant} to subtyping relation, which plays a great role in how we treat effect unification variables.
During computation of \texttt{solve\_constraints\_within} $\Gamma$ $\tau/\varepsilon$ we keep information about $variance$ of effect unification variables as a function $V$.
$$
    v \Coloneqq \oplus \mid \odot \mid \ominus \mid \times \qquad\qquad \text{(variance)}
$$
If each occurrence of a variable $\epsilon$ in $\tau/\varepsilon$
is \textit{covariant} (\textit{positive}) and \textit{contravariant} (\textit{negative}) in $\Gamma$, then $V(\epsilon) = \oplus$.
Similarly, if it appears only \textit{contravariantly} in $\tau/\varepsilon$
and only \textit{covariantly} in $\Gamma$, then $V(\epsilon) = \ominus$.
If it appears \textit{invariantly} (\textit{both} positively and negatively), then $V(\epsilon) = \odot$. Finally, if $\epsilon$ doesn't appear in type or environment at all, then $V(\epsilon) = \times$.

Because of the way we defined subtyping relation, we can \textit{shrink} any covariant effect and \textit{expand} any contravariant effect.
Consider type $\tau$ with some covariant effect $\varepsilon_\oplus$.
Clearly, for any $\varepsilon_\oplus' <: \varepsilon_\oplus$ we have
$ \tau[\varepsilon_\oplus' / \varepsilon_\oplus] <: \tau $.
Analogously, for any contravariant effect $\varepsilon_\ominus$ and 
any effect $\varepsilon_\ominus'$ such that $\varepsilon_\ominus <: \varepsilon_\ominus'$
we have
$ \tau[\varepsilon_\ominus' / \varepsilon_\ominus] <: \tau$.

With this intuition in mind, we can confidently resolve constraints
$\{\epsilon_1 <: I_2 * \epsilon_2\}$ if either $V(\epsilon_1) = \oplus$ or $V(\epsilon_2)=\ominus$
by assignment $[\epsilon_1 \mapsto I_2 * \epsilon_2]$, as it enables both
covariant variables to be the smallest and contravariant to grow the largest possible.

It is important to include the non-appearing variables in our algorithm as well,
as there are many unification variables that are generated along the way
that do not appear in the examined type nor environment explicitly,
but often do form \textit{chains} of subtyping constraints like so
(instances ommited for better visibility):
$$\epsilon_\ominus <: \epsilon_1 <: \dots <: \epsilon_n <: \epsilon_\oplus \text{ where } V(\epsilon_i) = \times$$
In such case, we want $\epsilon_\ominus$ to be \textit{the biggest} and $\epsilon_\oplus$ to be \textit{the smallest} possible, so we would like to deduce the substitution $S$ such that
$S\epsilon_\ominus = S\epsilon_1 = \dots = S\epsilon_n = S\epsilon_\oplus$,
as it guarantees that it is in fact the case.

\begin{lstlisting}
solve_within $\Gamma$ $\tau/ \varepsilon$ $C$ $S$ =
  $V :=$ gather_free_vars $S\Gamma$ $S\tau/S\varepsilon$
  while $\exists (I_1 * \epsilon_1 <: I_2 * \epsilon_2) \in S\:C.\: \epsilon_1 \neq \epsilon_2 \wedge V(\epsilon_1), V(\epsilon_2)$ matches
      | $\times,\oplus$ | $\ominus,\times$ | $\times,\odot$ | $\odot,\times$
      | $\oplus,\oplus$ | $\ominus,\ominus$ | $\odot,\odot$ $\rightarrow$
          $C := C \setminus \{ I_1 * \epsilon_1 <: I_2 * \epsilon_2 \}$;
          $S := $(expand $(I_1 \setminus I_2)$ $\epsilon_2$ $S$)$[\epsilon_1 \mapsto I_2 * \epsilon_2]$;
      | $\ominus,\oplus$ | $\ominus,\odot$ | $\odot,\oplus$ $\rightarrow$
          $C := C \setminus \{I_1 * \epsilon_1 <: I_2 * \epsilon_2 \}$;
          $S := $(expand $(I_1 \setminus I_2)$ $\epsilon_2$ $S$)$[\epsilon_1 \mapsto I_2 * \epsilon_2]$;
          $V := V[\epsilon_2 \mapsto \odot]$
  for $(\epsilon, \oplus) \in V$:
      $S := S[\epsilon\mapsto\iota]$
  return $C,S$
  
\end{lstlisting}
We conclude the description of algorithm by
defining function \textit{solve} from $\vdash_\textbf{Gen}$:
\begin{gather*}\begin{aligned}
\operatorname{solve}(\Gamma, \tau/\varepsilon, C, S) = 
    \texttt{solve\_within}\;\Gamma\;\tau / \varepsilon\;& C'\;S' \\
    \text{where } S', C' = \texttt{solve\_simple}\;& C\:S 
\end{aligned}
\end{gather*}
\vfill
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Example}
Now that we have a complete picture of how the inference algorithm works, let's take a look how constraints for \textit{compose} would be generated and resolved.
$$
\inferrule{
 \inferrule{
  \inferrule{
     \inferrule{
          \inference{ (f : t_f) \in \Gamma_1\ \operatorname{inst}(t_f) = t_f
          }{\gens[\Gamma_1]{f}{t_f}{\iota}{\emptyset}{id}}
       \ 
     \inferrule
      {
         \inference{(f : t_g) \in \Gamma_1\ \operatorname{inst}(t_g) = t_g}{\gens[\Gamma_1]{g}{t_g}{\iota}{\emptyset}{id}}
         \\
         \inference{(x : t_x) \in \Gamma_1\ \operatorname{inst}(t_x) = t_x}{\gens[\Gamma_1]{x}{t_x}{\iota}{\emptyset}{id}} 
      }{
         \gens[\Gamma_1]{(g\ x)}{t_1}{\epsilon_1}{C_1}{id}
      } 
      }{
     \quad 
     \gens[\Gamma_1]{f\ (g\ x)}{t_2}{\epsilon_2}{C_1}{id}
    }
   }{
    \gens[(f:t_f),(g:t_g)]{\lam f\ (g\ x)}{\arrow{t_x}{\epsilon_2}{t_2}}{\iota}{C_1}{id}
   }
  }{
  \gens[(f:t_f)]{\lam[g]\lam f\ (g\ x)}{\arrow{t_g}{}{\arrow{t_x}{\epsilon_2}{t_2}}}{\iota}{C_1}{id}
  }
}{
\gens[]{\lam[f]\lam[g]\lam f\ (g\ x)}{\arrow{t_f}{}{\arrow{t_g}{}{\arrow{t_x}{\epsilon_2}{t_2}}}}{\iota}{C_1}{id}
}
$$
\begin{align*}
C_1 & = \{\iota <: \epsilon_1, \iota <: \epsilon_1, t_g <: \arrow{t_x}{\epsilon_1}{t_1}\}
\\
C_2 & = \{
            \iota <: \epsilon_1, \iota <: \epsilon_1, t_g <: \arrow{t_x}{\epsilon_1}{t_1}, 
            \iota <: \epsilon_2, \epsilon_1 <: \epsilon_2, t_f <: \arrow{t_1}{\epsilon_2}{t_2}
        \}
\\
\Gamma_1 & = (f:t_f),(g:t_g),(x:t_x)
\\
  & \texttt{solve\_simple}\  C_1\ id = \\
  & \texttt{  solve\_simple}\ \{
            \epsilon_1 <: \epsilon_2,
           t_g <: \arrow{t_x}{\epsilon_1}{t_1}, 
           t_f <: \arrow{t_1}{\epsilon_2}{t_2}
        \}\ id = \\
  & \qquad \{\epsilon_1 <: \epsilon_2\} ; S_1\\
S_1 & = [t_g \mapsto \arrow{t_x}{\epsilon_1}{t_1}, t_f \mapsto \arrow{t_1}{\epsilon_2}{t_2}]
\\
  & \texttt{solve\_within}\ \emptyset\
    (\arrow{(\arrow{t_1}{\epsilon_2}{t_2})}{}{\arrow{(\arrow{t_x}{\epsilon_1}{t_1})}{}{\arrow{t_x}{\epsilon_2}{t_2}}})
    \ \iota \ \{\epsilon_1 <: \epsilon_2\} \ S_1 = \\
  & \quad V = [\epsilon_1 \mapsto \ominus, \epsilon_2 \mapsto \odot] ;\\
  & \texttt{return } \emptyset ; S_2
\\
S_2 & = [\epsilon_1 \mapsto \epsilon_2, t_g \mapsto \arrow{t_x}{\epsilon_1}{t_1}, t_f \mapsto \arrow{t_1}{\epsilon_2}{t_2}]
\\
& \operatorname{solve}(\emptyset, {\arrow{t_f}{}{\arrow{t_g}{}{\arrow{t_x}{\epsilon_2}{t_2}}}}/{\iota}, C_1, id) =
\\
& \quad \emptyset ; S_1[\epsilon_1 \mapsto \epsilon_2]
\\
\tau_1 & = \arrow{(\arrow{t_1}{\epsilon_2}{t_2})}{}{\arrow{(\arrow{t_x}{\epsilon_2}{t_1})}{}{\arrow{t_x}{\epsilon_2}{t_2}}} \\
& \operatorname{gen}(\emptyset, \tau_1) = \pi_1 \\
\pi_1 & =  \forall \alpha, \beta, \gamma, \delta .\ 
\arrow{(\arrow{\alpha}{\delta}{\beta})}{}{\arrow{(\arrow{\gamma}{\delta}{\alpha})}{}{\arrow{\gamma}{\delta}{\beta}
}}\\
\Gamma_2 & = (compose\ : \pi_1) 
\end{align*}
Notice that the inferred type is indeed the most general one, as discussed in \hyperlink{section.3.2}{Chapter 3}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Implementation}
The algorithm we described is accompanied by a software implementation,
written in pure OCaml.
Differences from $\vdash_\textbf{Gen}$ include that the built substitution is
applied globally \textit{on the fly}, rather than returned,
as the unification variables are implemented using memory references that are updated throughout the process.
By convention, all the occurrences \textit{universal variables} (denoted $'\tau\alpha$
or $'\varepsilon\alpha$) are quantified \textit{by default}
(the $\forall$-quantifier of type schemes is implicit).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Project structure}
Type system and expressions of the calculus are defined in \texttt{calculus.ml} file:
\begin{lstlisting}
type identifier = string
type instance = identifier
type instance = identifier

type $\text{'}$a univar = Free of identifier | Bound of $\text{'}$a

type typ =
  | Unit
  | Int
  | Arrow    of typ * typ * effect              (*  $\arrow{\tau_1}{\varepsilon}{\tau_2}\ $      *)
  | TypVar   of typ univar ref                  (*     $t$     *)
  | GenTyp   of identifier                      (*    $'\tau\alpha$      *)
  | Forall   of instance * signature * typ      (*  $\forall (a:\sigma).\ \tau$ *)
  | IllTyped

and signature = Error | State of typ          (* $ {Error} \mid {State}\ \tau $ *)

and effect =
  | Fixed    of instance set                      (*    $\ I\ \ $  *)
  | Flexible of instance set * effect univar ref  (*   $I * \epsilon\ $ *)
  | GenEff   of instance set * identifier         (*  $I * '\varepsilon\alpha$ *)
type expr =
  | Nil                                              (*  $()$   *)
  | I       of int                                   (*  $n$   *)
  | V       of var                                   (*  $x$   *)
  | Lam     of var * expr                            (* $\lam e$ *)
  | Fun     of var * var * expr                  (* $\fun e$ *)
  | Let     of var * expr * expr            (* $\textbf{let }x = e_1 \textbf{ in } e_2$ *)
  | App     of expr * expr                           (* $e_1\ e_2$ *)
  | Op      of instance * op * expr                  (* $op_a\ e$ *)
  | Handle  of instance * expr * handler       (* $\textbf{handle}_a \: e\: \{h\}$ *)
  | ILam    of instance * signature * expr        (* $\lam[(a:\sigma)] e$ *)
  | IApp    of expr * instance                        (* $e\ a$ *)
  | UHandle of expr * handler                  (*  $\textbf{handle} \: e\: \{h\}$ *)
  | UOp     of op * expr                              (* $op\ e$ *)
  
and op = Raise | Get | Put

and handler = (op * var * var * expr) list * var * expr
                                  (* $[(op,\ x,\ k,\ e)]\ ;\ \textbf{return } x.\ e_r$ *)
type op_type = typ * typ                           (* $\tau_1 \Rightarrow \tau_2$ *)

type type_effect = typ * effect                       (* $\tau/\varepsilon$ *)
\end{lstlisting}
The \texttt{UHandle} and \texttt{UOp} stand for \textit{unnamed} operators and handlers,
used without declaring an instance.
The way they're dealt with is by simple desugaring into named handlers with
fresh instance.

Judgement $\vdash_\textbf{Gen}$ is implemented in \texttt{inference.ml} 
by the \texttt{infer} sub-procedure of the \texttt{infer\_type\_with\_env}  function,
which returns the type after resolving constraints, using procedures described in \hyperlink{section.4.2}{Chapter 4}.
Auxiliary procedures, such as the union-find operations and set operations are included in 
\texttt{utils.ml}.

\begin{lstlisting}
type $\text{'}$a constraints = ($\text{'}$a * $\text{'}$a) list

type $\text{'}$a environment = (identifier * $\text{'}$a) list
type env = typ environment
type ienv = signature environment

val solve_simple : typ constraints -> effect constraints -> effect constraints

val solve_within : env -> type_effect -> effect constraints -> effect constraints

val infer_type_with_env : env -> ienv -> expr -> env * type_effect
\end{lstlisting}
The \texttt{env}, returned by \texttt{infer\_type\_with\_env} containts
types of variables bound by \textit{let-constructs} in \texttt{expr}, included for meaningful examples printing.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Tutorial}
File \texttt{examples.ml} contains a few meaningful examples of type inference.
Simply running \texttt{ocamlbuild examples.byte} in project's root directory
should build the executable and all its dependencies.
By convention we print unresolved unification variables with \texttt{?} sign.
Below, we have included output of selected examples.
\begin{lstlisting}
$\text{\$}$> ./examples.byte 
$\vdash$ $\lambda$e:Error. $\lambda$x. raise_e x : $\forall$e:Error. Unit -{e}-> ?$\tau$1 / $\iota$

$\vdash$ handle
    put 21
  {put v k. k () | get () k. k 37 | return x. x} : Unit / $\iota$

$\vdash$ $\lambda$x. $\lambda$y. $\lambda$z. (x z) (y z) : (?$\tau$2 -{?$\varepsilon$2}-> ?$\tau$4 -{?$\varepsilon$2}-> ?$\tau$5) -> (?$\tau$2 {?$\varepsilon$2}-> ?$\tau$4) -> ?$\tau$2 -{?$\varepsilon$2}-> ?$\tau$5 / $\iota$

$\vdash$ let id = $\lambda$x. x in id id : ?$\tau$2 -> ?$\tau$2 / $\iota$
  (id : $\text'\tau$a -> $\text'\tau$a)

$\vdash$ let apply =
    $\lambda$f. $\lambda$x. f x
  in apply ($\lambda$x. x) : ?$\tau$4 -> ?$\tau$4 / $\iota$
  (apply : ($\text'\tau$a -{$\text'\varepsilon$b}-> $\text'\tau$c) -> $\text'\tau$a -{$\text'\varepsilon$b}-> $\text'\tau$c)

$\vdash$ let fix =
    fun fix f. f (fix f)
  in fix ($\lambda$x. $\lambda$y. $\lambda$z. 2) : ?$\tau$6 -> ?$\tau$7 -> Int / $\iota$
  (fix : ($\text'\tau$a -{$\text'\varepsilon$b}-> $\text'\tau$a) -{$\text'\varepsilon$b}-> $\text'\tau$a)

$\vdash$ let update =
    $\lambda$s:State a. $\lambda$f. put_s (f (get_s ()))
  in handle_b
    ($\lambda$(). get_b ()) ((update<b>) ($\lambda$x. x))
  { get () k. $\lambda$c. (k c) c
  | put v k. $\lambda$c. (k ())   v
  | return x. $\lambda$c. x} : ?$\tau$15 -> ?$\tau$15 / $\iota$
  (update : $\forall$s:State a. (a -{s,$\text'\varepsilon$a}-> a) -{s,$\text'\varepsilon$a}-> Unit)

$\vdash$ let move_map =
    $\lambda$from:State a. $\lambda$to:State b. $\lambda$f. put_to (f (get_from ()))
  in 1 : Int / $\iota$
  (move_map : $\forall$from:State a. $\forall$to:State b.
        (a -{from,$\text'\varepsilon$a}-> b) -{from,to,$\text'\varepsilon$a}-> Unit)
\end{lstlisting}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusions and future work}
Based on the \textit{original calculus}, we have proposed a subset calculus and type system
that enabled us to implement ML-polymorphism for decidable type inference.
The algorithm we have shown is an refinement of algorithm $\mathbb{W}$
with simple subtyping relation, with extended calculus and type system for
algebraic effects.

Let-polymorphism allows programmers to omit implicit \textit{type-lambdas}
(denoted by $\Lambda$) and implicit type instantiating, 
making programmers' lives easier and the code more readable.
Main focus of future work should be focused on how to achieve
similar flexibility for instance lambdas.
The problem is somewhat more difficult,
as whether we use the \textit{type-lambda} or not does not affect
the semantics of expression, while the presence or absence
of type lambdas certainly change it.

The way we resolve effect constraints is not a formally sound method.
Presence of subtyping relation in type inference does not impose
much added difficulty from the theoretical point of view,
but it does make the practical constraint solving a much harder task.
Because of this relation, constraints describing effects
form a partially ordered set, and finding the most general
substitution that satisfies it, is beyond this work.
Proving this property is essential to arguing about the
principal type, which is highly desirable for an inference algorithm,
as it enables programmers to be sure that even if they skip all
the type annotations, the inferred type will not be less general.
This  property  is  essential  for full  type  inference.

A few works had been written the topic of type inference with subtyping,
proving its difficulty, including the  Ph.D. thesis of François Pottier\cite{francois}.
However, our subtyping relation is much simpler than the one in his work,
then maybe, with just the right amount of depth and a bit of ingenuity,
studying the topological properties of subtyping relation,
could lead to some simpler, yet \textit{sound} solution.

Nonetheless, the \textit{heuristic} approach we described
produced desired results for all the examples we have tried.
However, it may be possible that there is type and constraint set that our method
fails to generate the \textit{most general unifier}, but given short time window for
this work we were not able to find such example.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% BIBLIOGRAFIA

\begin{thebibliography}{1}
\bibitem{hindley}
    J. Roger Hindley. 1969. The Principal Type-Scheme of an Object in Combinatory Logic. Transactions of the American Mathematical Society, 146, 29-60. \url{https://doi.org/10.2307/1995158}
\bibitem{milner}
    Robin Milner. 1978.
    A theory of type polymorphism in programming,
    Journal of Computer and System Sciences,
    Volume 17, Issue 3,
    1978,
    Pages 348-375,
    ISSN 0022-0000,
     \url{https://doi.org/10.1016/0022-0000(78)90014-4}.
\bibitem{principal}
    Luis Damas and Robin Milner. 1982. Principal type-schemes for functional programs. In \textit{Proceedings of the 9th ACM SIGPLAN-SIGACT symposium on Principles of programming languages} (\textit{POPL '82}). Association for Computing Machinery, New York, NY, USA, 207–212. \url{https://doi.org/10.1145/582153.582176}
\bibitem{talpin}
    Jean Pierre Talpin, Pierre Jouvelot. 1992. The type and effect discipline, \textit{Proceedings of the Seventh Annual IEEE Symposium on Logic in Computer Science}, Santa Cruz, CA, USA, 1992, pp. 162-173, \url{https://doi.org/10.1109/LICS.1992.185530}.
\bibitem{binders-labels}
    Dariusz Biernacki, Maciej Piróg, Piotr Polesiuk, and Filip Sieczkowski. 2019. Binders by day, labels by night: effect instances via lexically scoped handlers. \textit{Proc. ACM Program. Lang.} 4, POPL, Article 48 (January 2020), 29 pages. \url{https://doi.org/10.1145/3371116}
\bibitem{emlti}
    Pottier François and Didier Rémy. 2005.
    The Essence of ML Type Inference.
    MIT press, Chapter 10 of \textit{Advanced topics in types and programming languages}.
\bibitem{francois}
    François Pottier. 1998. Type Inference in the Presence of Subtyping: from Theory to Practice. [Research
    Report] RR-3483, INRIA. ffinria-00073205f
\end{thebibliography}
\end{document}
